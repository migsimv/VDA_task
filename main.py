# -*- coding: utf-8 -*-
"""Copy of Programuotojų užduotys kandidatams 2023

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15PbDpy-E-YhsR5pojv-_W9gYMp273aBS

**Programuotojų užduotys**

# Naudinga informacija
- Prieš pradėdami spręsti, **pasidarykite** šio Google Colab notebooko **kopiją**.
- Atlikę užduotis, suteikite mums teises ir atsiųskite nuorodą.
- Vertinsime sprendimo atitikimą užduoties sąlygoms, programavimo stilių, darbo su duomenimis įgūdžius.

---

# Reikalavimai ir resursai

Šioms užduotims atlikti būtina naudoti **Python** ir **PySpark** biblioteką.

## Resursai

1) Nacionalinės švietimo agentūros lentelė su pedagogų kvalifikacija pagal savivaldybes. Lentelės aprašymas: https://data.gov.lt/dataset/pedagogu-kvalifikacija

2) Elektromobilių įkrovos stotelių duomenys. Aprašymas: https://maps.eismoinfo.lt/portal/apps/sites/#/npp/pages/charge

---

# Užduotys

## Pedagogų registras _(junior)_

1. Sutvarkyti pedagogų kvalifikacijos lentelę:
  - Jei tekstiniuose laukuose reikšmė yra `null`, ją pakeisti tekstu "Nenurodyta".
  - Stulpelius pervadinti į `snake_case` stilių.
  - Nustatyti tinkamus stulpelių duomenų tipus.
  - Papildomai atlikti transformacijas, kurios, jūsų manymu, būtų naudingos (pvz., išrinkti tik tokius stulpelius, kurie bus naudojami).

2. Paruošti lentelę, rodančią mokytojų kiekį kiekvienoje savivaldybėje (naudokite tik tokius įrašus, kurių lauko `pd_pareigu_grupe` reikšmė yra "Mokytojai") ir visoje Lietuvoje.


## Pedagogai 2 _(mid-level)_

1. Sutvarkyti pedagogų kvalifikacijos lentelę:
  - Jei tekstiniuose laukuose reikšmė yra `null` arba _falsy_ (pasidomėkite, ką tai reiškia Python aplinkoje), ją pakeisti tekstu "Nenurodyta".
  - Stulpelių vardus __automatizuotai__ pervadinti į `snake_case` stilių, taip, kad jūsų sprendimas veiktų ir jei lentelė pasipildytų su daugiau analogiškai užvardintų stulpelių.
  - Nustatyti tinkamus stulpelių duomenų tipus.
  - Papildomai atlikti transformacijas, kurios, jūsų manymu, būtų naudingos (pvz., išrinkti tik tokius stulpelius, kurie bus naudojami).

2. Paruošti __vieną__ lentelę, rodančią:
  - mokytojų kiekį kiekvienoje savivaldybėje (naudokite tik tokius įrašus, kurių lauko `pd_pareigu_grupe` reikšmė yra "Mokytojai") ir visoje Lietuvoje, ir
  - mokytojų procentinę dalį (nuo visos Lietuvos), tenkančią kiekvienai savivaldybei.

3. Kaip kitaip būtų galima saugoti šiuos duomenis? Parašykite transformaciją ir pakomentuokite, kokiomis sąlygomis jūsų pasiūlytas variantas būtų geresnis už naudojamą dabar. O kokiomis sąlygomis geriau geriau naudoti esamą formatą?


## Elektromobilių įkrovos stotelių duomenys _(senior)_

1. Paversti duomenis iš XML formato į `Spark DataFrame` lentelę ir sutvarkyti taip:
  - Jei tekstiniuose laukuose reikšmė yra `null` arba _falsy_, ją pakeisti tekstu "Nenurodyta".
  - Stulpelių pavadinimus __automatizuotai__ pervadinti `snake_case` stiliumi.
  - Nustatyti tinkamus stulpelių duomenų tipus.
2. Parašyti transformaciją, kuri iš šios lentelės sugeneruotų lentelę, parodančią visus kiekviename stulpelyje rastus simbolius. Išvesties lentelė turi atrodyti taip:
  - Du stulpeliai, `name` ir `contents`.
  - Kiekvienas pradinės lentelės stulpelis gauna atskirą eilutę, stulpelio pavadinimas nurodomas `name`.
  - Visi kiekviename stulpelyje rasti unikalūs simboliai parodomi `contents` stulpelyje.
3. Pakomentuokite apie 2-ame klausime jūsų pasirinkto sprendimo efektyvumą: su kokiais duomenų rinkiniais jūsų algoritmas būtų optimalus, o kada būtų geresnis kitoks sprendimas?
"""

# !pip install pyspark

from pyspark.sql import SparkSession
from pyspark.sql import functions as F
import requests

url_pedagogai = "https://data.gov.lt/dataset/12/download/4468/Pedagogu%20kvalifikacija-12-lt-lt.csv"
url_gyventoju_registras = "https://www.registrucentras.lt/aduomenys/?byla=01_gr_open_amzius_lytis_pilietybes_sav_r1.csv"

spark = SparkSession.builder.appName("Read CSV from URL").getOrCreate()

def csv_to_df(spark, url, delimiter="\t"):

    data = requests.get(url)
    rdd = spark.sparkContext.parallelize(data.iter_lines())
    # Use the first row as the header
    header = rdd.first()
    rdd = rdd.filter(lambda line: line != header)
    # Create the PySpark DataFrame
    df = spark.createDataFrame(rdd.map(lambda line: line.decode("utf-8").split(delimiter)), header.decode("utf-8").split(delimiter))
    return df

df_pedagogai = csv_to_df(spark, url_pedagogai, "\t")
df_gr = csv_to_df(spark, url_gyventoju_registras, "|")

# Naudojimo pavyzdys:
df_pedagogai.show(3)
df_gr.show(3)